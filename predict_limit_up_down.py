import tushare as ts
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import os
import time
import pickle
import smtplib
from email.mime.text import MIMEText
from email.header import Header
from sktime.classification.interval_based import TimeSeriesForestClassifier
from sklearn.ensemble import GradientBoostingClassifier  # æ·»åŠ åˆ°æ–‡ä»¶é¡¶éƒ¨çš„å¯¼å…¥éƒ¨åˆ†

# è®¾ç½® tushare token
ts.set_token('c0f992e8369579bfec7bf8481dc0bcc304ac66ab5b1dd12c1d154325')
pro = ts.pro_api()

CACHE_DIR = 'cache'
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)

def save_to_cache(filename, data):
    with open(os.path.join(CACHE_DIR, filename), 'wb') as f:
        pickle.dump(data, f)

def load_from_cache(filename):
    filepath = os.path.join(CACHE_DIR, filename)
    if os.path.exists(filepath):
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    return None

def get_daily_limit_data(trade_date, fields):
    """è·å–å•æ—¥æ¶¨è·Œåœæ•°æ®"""
    return pro.limit_list_d(trade_date=trade_date, fields=fields)

def get_limit_ths_data(start_date, end_date, fields):
    """è·å–æ¶¨è·Œåœæ•°æ®"""
    return pro.limit_list_ths(
        start_date=start_date,
        end_date=end_date,
        fields=','.join(fields)
    )

def get_market_data(start_date, end_date):
    """è·å–å¸‚åœºæ•°æ®ï¼ŒåŒ…æ‹¬ä¸­å°ç›˜æŒ‡æ•°å’Œæ¶¨è·Œåœç‚¸æ¿æ•°æ®"""
    try:
        # è·å–äº¤æ˜“æ—¥å†å¹¶æŒ‰æ—¥æœŸæ­£åºæ’åˆ—
        trade_cal = pro.trade_cal(start_date=start_date, end_date=end_date)
        trade_dates = trade_cal[trade_cal['is_open'] == 1]['cal_date'].sort_values(ascending=False).tolist()
        
        # æ‰“å°æ—¥æœŸèŒƒå›´ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"è·å–æ•°æ®çš„æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
        print(f"æœ€æ–°äº¤æ˜“æ—¥: {trade_dates[0]}")
        print(f"æœ€æ—©äº¤æ˜“æ—¥: {trade_dates[-1]}")
        
        # è·å–ä¸­å°ç›˜æŒ‡æ•°æ•°æ®ï¼ˆä¸­è¯500æŒ‡æ•°ï¼‰
        df_index = pro.index_daily(ts_code='000852.SH', 
                                 start_date=start_date, 
                                 end_date=end_date)
        
        all_limit_data = []
        for trade_date in trade_dates:
            cache_filename = f"limit_data_{trade_date}.pkl"
            daily_stat = load_from_cache(cache_filename)
            if daily_stat is None:
                # è·å–æ¶¨åœã€ç‚¸æ¿å’Œè·Œåœæ•°æ®
                df_up = pro.kpl_list(trade_date=trade_date, tag='æ¶¨åœ', 
                                   fields='ts_code,name,trade_date,tag,status')
                time.sleep(1)
                df_broken = pro.kpl_list(trade_date=trade_date, tag='ç‚¸æ¿', 
                                       fields='ts_code,name,trade_date,tag,status')
                time.sleep(1)
                df_down = pro.kpl_list(trade_date=trade_date, tag='è·Œåœ', 
                                     fields='ts_code,name,trade_date,tag,status')
                time.sleep(1)
                
                # ç»Ÿè®¡å½“æ—¥æ•°æ®
                daily_stat = {
                    'trade_date': trade_date,
                    'up_limit': len(df_up),             # æ¶¨åœå®¶æ•°
                    'down_limit': len(df_down),         # è·Œåœå®¶æ•°
                    'broken_limit': len(df_broken),     # ç‚¸æ¿å®¶æ•°
                    'pure_limit': len(df_up[df_up['status'].str.contains('é¦–æ¿', na=False)]),  # é¦–æ¿å®¶æ•°
                    'avg_break_times': len(df_broken) / len(df_up) if len(df_up) > 0 else 0  # ç‚¸æ¿ç‡
                }
                save_to_cache(cache_filename, daily_stat)
                print(f"å·²è·å– {trade_date} çš„æ¶¨è·Œåœæ•°æ®å¹¶ä¿å­˜åˆ°ç¼“å­˜")
            # else:
            #     print(f"ä»ç¼“å­˜ä¸­åŠ è½½ {trade_date} çš„æ¶¨è·Œåœæ•°æ®")
            
            all_limit_data.append(daily_stat)
        
        # è½¬æ¢ä¸ºDataFrame
        daily_stats = pd.DataFrame(all_limit_data)
        daily_stats = daily_stats.set_index('trade_date')
        
        print(f"æˆåŠŸè·å– {start_date} è‡³ {end_date} çš„å¸‚åœºæ•°æ®")
        print(f"å…±å¤„ç† {len(daily_stats)} ä¸ªäº¤æ˜“æ—¥")
        
        return df_index, daily_stats
        
    except Exception as e:
        print(f"è·å–æ•°æ®å¤±è´¥: {str(e)}")
        return None

def prepare_features(df_index, daily_stats, window=7):
    # æŒ‰ç…§æ—¶é—´å‡åºæ’åºæŒ‡æ•°æ•°æ®
    df_index['trade_date'] = pd.to_datetime(df_index['trade_date'])
    df_index = df_index.sort_values('trade_date')
    df_index['trade_date'] = df_index['trade_date'].dt.strftime('%Y%m%d')
    df_index = df_index.set_index('trade_date')
    
    # æŒ‰ç…§æ—¶é—´å‡åºæ’åºæ¶¨è·Œåœæ•°æ®
    daily_stats.index = pd.to_datetime(daily_stats.index)
    daily_stats = daily_stats.sort_index()
    daily_stats.index = daily_stats.index.strftime('%Y%m%d')
    
    # åˆå¹¶æ•°æ®
    df = df_index.merge(daily_stats, left_index=True, right_index=True)
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {df.index[0]} è‡³ {df.index[-1]}")
    print(f"æ•°æ®æ¡æ•°: {len(df)}")
    
    # è®¡ç®—æŒ‡æ•°æ¶¨è·Œå¹…ï¼Œå¹¶å°†NaNå¡«å……ä¸º0
    df['index_return'] = df['close'].pct_change().fillna(0)
    
    # æ·»åŠ æ›´å¤šç‰¹å¾ï¼Œå¤„ç†é™¤é›¶æƒ…å†µ
    total_limits = df['up_limit'] + df['down_limit']
    df['up_limit_ratio'] = (df['up_limit'] / total_limits).fillna(0)
    df['down_limit_ratio'] = (df['down_limit'] / total_limits).fillna(0)
    
    # åˆ›å»ºç‰¹å¾
    feature_cols = []
    for i in range(window):
        # ä½¿ç”¨shiftåˆ›å»ºç‰¹å¾å¹¶å¡«å……NaNä¸º0
        df[f'up_limit_t{i+1}'] = df['up_limit'].shift(i+1).fillna(0)
        df[f'down_limit_t{i+1}'] = df['down_limit'].shift(i+1).fillna(0)
        df[f'broken_limit_t{i+1}'] = df['broken_limit'].shift(i+1).fillna(0)
        df[f'pure_limit_t{i+1}'] = df['pure_limit'].shift(i+1).fillna(0)
        df[f'avg_break_times_t{i+1}'] = df['avg_break_times'].shift(i+1).fillna(0)
        
        feature_cols.extend([
            f'up_limit_t{i+1}', 
            f'down_limit_t{i+1}',
            f'broken_limit_t{i+1}',
            f'pure_limit_t{i+1}',
            f'avg_break_times_t{i+1}'
        ])
    
    # åˆ›å»ºç›®æ ‡å˜é‡ï¼Œå°†NaNå¡«å……ä¸º0
    df['target'] = (df['index_return'].shift(-1) > 0).astype(int).fillna(0)
    
    return df, feature_cols

def train_model(df, feature_cols):
    """ä½¿ç”¨TimeSeriesForestæ¨¡å‹è¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹"""
    X = df[feature_cols]
    y = df['target']
    
    # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = int(len(df) * 0.8)
    X_train = X[:train_size]
    X_test = X[train_size:]
    y_train = y[:train_size]
    y_test = y[train_size:]
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # æ„å»ºTimeSeriesForestæ¨¡å‹
    model = TimeSeriesForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=1  # å•çº¿ç¨‹è¿è¡Œ
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    model.fit(X_train_scaled, y_train)
    print("æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # è¯„ä¼°æ¨¡å‹
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    
    return model, scaler, train_score, test_score

def send_email(report):
    # é‚®ä»¶å‘é€é…ç½®
    sender = '652433935@qq.com'  # å‘ä»¶äººé‚®ç®±
    receiver = '652433935@qq.com'  # æ”¶ä»¶äººé‚®ç®±
    smtp_server = 'smtp.qq.com'  # SMTPæœåŠ¡å™¨
    smtp_port = 587  # SMTPç«¯å£
    username = '652433935@qq.com'  # ç™»å½•ç”¨æˆ·å
    password = 'toepnllhqbfbbffc'  # ç™»å½•å¯†ç æˆ–æˆæƒç 
    
    try:
        # åˆ›å»ºé‚®ä»¶å†…å®¹
        message = MIMEText(report, 'plain', 'utf-8')
        message['From'] = Header(sender)
        message['To'] = Header(receiver)
        message['Subject'] = Header('è‚¡ç¥¨é¢„æµ‹ç»“æœ')
        
        # å‘é€é‚®ä»¶
        server = smtplib.SMTP(smtp_server, smtp_port)
        server.starttls()  # ä½¿ç”¨TLSåŠ å¯†
        server.login(username, password)
        server.sendmail(sender, [receiver], message.as_string())
        server.quit()
        print("é‚®ä»¶å‘é€æˆåŠŸ")
    except Exception as e:
        print(f"é‚®ä»¶å‘é€å¤±è´¥: {str(e)}")

def find_optimal_days():
    """å¯»æ‰¾æœ€ä¼˜çš„å†å²æ•°æ®å¤©æ•°"""
    best_score = 0
    best_days = 30
    best_model = None
    best_scaler = None
    results = []
    
    # æµ‹è¯•ä¸åŒçš„å¤©æ•°
    for days in range(90, 180, 5):  # ä»30å¤©åˆ°90å¤©ï¼Œæ¯æ¬¡å¢åŠ 3å¤©
        end_date = datetime.now().strftime('%Y%m%d')
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
        
        print(f"\næµ‹è¯• {days} å¤©çš„æ•°æ®:")
        # è·å–æ•°æ®
        df_index, daily_stats = get_market_data(start_date, end_date)
        if df_index is None or daily_stats is None:
            continue
            
        # å‡†å¤‡ç‰¹å¾
        df, feature_cols = prepare_features(df_index, daily_stats)
        
        # è®­ç»ƒæ¨¡å‹
        model, scaler, train_score, test_score = train_model(df, feature_cols)
        
        print(f"å¤©æ•°: {days}")
        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.4f}")
        
        results.append({
            'days': days,
            'train_score': train_score,
            'test_score': test_score
        })
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if test_score > best_score:
            best_score = test_score
            best_days = days
            best_model = model
            best_scaler = scaler
    
    # æ‰“å°æ‰€æœ‰ç»“æœ
    results_df = pd.DataFrame(results)
    print("\nå…¨éƒ¨æµ‹è¯•ç»“æœ:")
    print(results_df.sort_values('test_score', ascending=False))
    
    print(f"\næœ€ä¼˜å¤©æ•°: {best_days}")
    print(f"æœ€ä¼˜æµ‹è¯•é›†å‡†ç¡®ç‡: {best_score:.4f}")
    
    return best_days, best_model, best_scaler, best_score

def main():
    # å¯»æ‰¾æœ€ä¼˜å¤©æ•°
    best_days, model, scaler, best_score = find_optimal_days()
    
    # ä½¿ç”¨æœ€ä¼˜å¤©æ•°é‡æ–°è·å–æ•°æ®è¿›è¡Œé¢„æµ‹
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=best_days)).strftime('%Y%m%d')
    
    # è·å–æ•°æ®
    df_index, daily_stats = get_market_data(start_date, end_date)
    df, feature_cols = prepare_features(df_index, daily_stats)
    
    # ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
    try:
        # è·å–æœ€æ–°æ•°æ®
        latest_data = df[feature_cols].iloc[-1:].values
        latest_data_scaled = scaler.transform(latest_data)
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        latest_date = df.index[-1]
        if isinstance(latest_date, (np.int64, np.int32, int)):
            latest_date = str(latest_date)
            if len(latest_date) != 8:  # å¦‚æœä¸æ˜¯YYYYMMDDæ ¼å¼
                print("é”™è¯¯ï¼šæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®")
                return
    
        report = ""
        
        try:
            # æ˜å¤©çš„é¢„æµ‹
            tomorrow = datetime.strptime(latest_date, '%Y%m%d') + timedelta(days=1)
            tomorrow_date = tomorrow.strftime('%Y%m%d')
            prediction_tomorrow = model.predict(latest_data_scaled)
            probability_tomorrow = model.predict_proba(latest_data_scaled)
            
            # åå¤©çš„é¢„æµ‹
            day_after = tomorrow + timedelta(days=1)
            day_after_date = day_after.strftime('%Y%m%d')
            prediction_day_after = model.predict(latest_data_scaled)
            probability_day_after = model.predict_proba(latest_data_scaled)
            
            report += "\n=== ä¸­å°ç›˜è‚¡å¸‚åœºé¢„æµ‹ç»“æœ ===\n"
            report += f"\né¢„æµ‹åŸºå‡†æ—¥æœŸ: {latest_date}\n"
            report += f"æµ‹è¯•é›†å‡†ç¡®ç‡: {best_score:.4f}\n"
            
            report += f"\næ˜å¤© ({tomorrow_date}) é¢„æµ‹:\n"
            report += "ä¸Šæ¶¨\n" if prediction_tomorrow[0] == 1 else "ä¸‹è·Œ\n"
            report += f"ä¸Šæ¶¨æ¦‚ç‡: {probability_tomorrow[0][1]:.2%}\n"
            report += f"ä¸‹è·Œæ¦‚ç‡: {probability_tomorrow[0][0]:.2%}\n"
            
            report += f"\nåå¤© ({day_after_date}) é¢„æµ‹:\n"
            report += "ä¸Šæ¶¨\n" if prediction_day_after[0] == 1 else "ä¸‹è·Œ\n"
            report += f"ä¸Šæ¶¨æ¦‚ç‡: {probability_day_after[0][1]:.2%}\n"
            report += f"ä¸‹è·Œæ¦‚ç‡: {probability_day_after[0][0]:.2%}\n"
            
            # ä¹°å…¥å»ºè®®
            report += "\n=== ä¹°å…¥å»ºè®® ===\n"
            if (prediction_tomorrow[0] == 1 and probability_tomorrow[0][1] >= 0.6 and
                prediction_day_after[0] == 1 and probability_day_after[0][1] >= 0.55):
                report += "å»ºè®®ä¹°å…¥ âœ…\n"
                report += "ç†ç”±ï¼šè¿ç»­ä¸¤å¤©çœ‹æ¶¨ï¼Œä¸”æ˜å¤©ä¸Šæ¶¨æ¦‚ç‡è¾ƒé«˜\n"
            elif prediction_tomorrow[0] == 1 and probability_tomorrow[0][1] >= 0.7:
                report += "å¯ä»¥è€ƒè™‘ä¹°å…¥ ğŸ¤”\n"
                report += "ç†ç”±ï¼šæ˜å¤©å¼ºåŠ¿ä¸Šæ¶¨æ¦‚ç‡é«˜\n"
            else:
                report += "æš‚ä¸å»ºè®®ä¹°å…¥ âŒ\n"
                report += "ç†ç”±ï¼šä¸Šæ¶¨æ¦‚ç‡ä¸ç¡®å®šæˆ–è¶‹åŠ¿ä¸æ˜æ˜¾\n"
            
            report += "\né£é™©æç¤ºï¼šæ¨¡å‹é¢„æµ‹ä»…ä¾›å‚è€ƒï¼Œè¯·ç»“åˆå…¶ä»–å› ç´ ç»¼åˆåˆ¤æ–­\n"
            
        except ValueError as e:
            print(e)
            report += f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}\n"
            report += f"å½“å‰æ—¥æœŸå€¼: {latest_date}\n"
            report += "è¯·ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸º'YYYYMMDD'\n"
        except Exception as e:
            report += f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\n"
        print(report)
        # å‘é€é‚®ä»¶
        # send_email(report)
    
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
